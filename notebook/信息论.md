# 信息论常见概念
## 熵
随机分布的熵度量了随机分布包含的信息量的多少，通常来说，熵越大，随机分布越不确定(例如: 均匀分布)，熵越小， 随机分布越确定(例如: 高斯分布)。计算公式：$H(X)=-\sum_{i=1}^{n}p(x_i)logp(x_i)$  
## KL散度
KL散度也叫信息熵，用来度量两个概率分布之间的差异(不能称之为距离，不满足对称性和三角不等式)。KL散度越大，差异越大，反之越小，计算公式:$KL(p||q)=\sum_{i=1}^{n}p(x_i)log\frac{p(x_i)}{q(x_i)}$  
## 互信息
互信息用来度量两个随机变量之间的关联程度，互信息越大，依赖程度越大，反之越小。计算公式:$I(X;Y)=\sum_{x,y}p(x,y)log\frac{p(x,y)}{p(x)p(y)}=H(X)-H(X|Y)=H(Y)-H(Y|X)$
**性质**
1.非负性  
$I(X;Y)\geq0$
2.对称性
$I(X;Y)=I(Y;X)$
3.与条件熵和联合熵的关系
$I(X;Y)=H(X)-H(X|Y)
       =H(Y)-H(Y|X)
       =H(X)+H(Y)-H(X,Y)
       =H(X,Y)-H(X|Y)-H(Y|X)$
4.与KL散度的关系
$I(X;Y)=\sum_{y}p(y)\sum_{x}p(x|y)log\frac{p(x|y)}{p(y)}  
       =\sum_{y}p(y)D_{KL}(p(x|y)||p(x))  
       $